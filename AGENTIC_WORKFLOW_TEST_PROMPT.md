# Agentic Workflow Testing Prompt

Use this comprehensive prompt to test your Cowork agentic workflow end-to-end. Test each scenario systematically and verify all components are working correctly.

---

## ğŸ¯ Test Scenario 1: Basic Agent Task Execution

**Objective**: Verify basic agent can receive, process, and respond to a simple task.

**Test Prompt**:
```
Create a simple Python script that prints "Hello, Cowork!" and saves it to a file called hello.py. Then execute it and show me the output.
```

**Expected Behavior**:
- âœ… Agent receives the task
- âœ… Task is decomposed into subtasks (if needed)
- âœ… Agent uses file_write tool to create hello.py
- âœ… Agent uses terminal/code_execution tool to run the script
- âœ… Output is streamed via SSE
- âœ… Task completes successfully
- âœ… File artifact is created and visible

**What to Check**:
- [ ] SSE events: `confirmed`, `create_agent`, `assign_task`, `activate_toolkit`, `deactivate_toolkit`, `end`
- [ ] Tool calls appear in workflow diagram
- [ ] File appears in artifacts/snapshots
- [ ] Task state transitions: waiting â†’ running â†’ done

---

## ğŸ”§ Test Scenario 2: Tool Calling Validation

**Objective**: Verify agent can use multiple tools correctly.

**Test Prompt**:
```
1. Search the web for "Python best practices 2024"
2. Create a markdown file summarizing the top 3 practices
3. Read the file back and display its contents
```

**Expected Behavior**:
- âœ… Agent uses `search` or `browser` tool
- âœ… Agent uses `file_write` tool to create markdown file
- âœ… Agent uses `file` tool to read the file
- âœ… All tool calls are logged and visible
- âœ… Results are properly formatted

**What to Check**:
- [ ] Multiple `activate_toolkit`/`deactivate_toolkit` events
- [ ] Tool results appear in agent messages
- [ ] File is created with correct content
- [ ] No tool execution errors

---

## ğŸŒ Test Scenario 3: Multi-Agent Workflow

**Objective**: Verify task decomposition and multi-agent coordination.

**Test Prompt**:
```
Build a simple web scraper that:
1. Searches for "AI agent frameworks" 
2. Extracts the top 5 results
3. Creates a comparison table in markdown
4. Generates a summary report

Break this down into subtasks and use appropriate agents for each part.
```

**Expected Behavior**:
- âœ… Task is decomposed into subtasks (`to_sub_tasks` event)
- âœ… Different agents are assigned (search_agent, document_agent)
- âœ… Subtasks execute in parallel or sequence
- âœ… Results are aggregated
- âœ… Final report is generated

**What to Check**:
- [ ] `to_sub_tasks` event with multiple subtasks
- [ ] Multiple `create_agent` events for different agent types
- [ ] `assign_task` events showing task-agent mapping
- [ ] Workflow diagram shows task tree structure
- [ ] All subtasks complete successfully

---

## ğŸ” Test Scenario 4: Provider Validation

**Objective**: Verify model provider validation works correctly.

**Test Steps**:
1. Go to Settings â†’ Providers
2. Add a new provider (e.g., OpenAI with test API key)
3. Click "Validate" button
4. Test with both valid and invalid keys

**Expected Behavior**:
- âœ… Validation endpoint is called with auth
- âœ… Valid API key: Returns `is_valid: true`, `is_tool_calls: true`
- âœ… Invalid API key: Returns `is_valid: false` with error message
- âœ… Tool call support is verified
- âœ… Error messages are user-friendly

**What to Check**:
- [ ] POST `/model/validate` requires authentication
- [ ] Token refresh works if token expires
- [ ] Validation response includes `is_tool_calls` field
- [ ] UI shows validation status correctly
- [ ] Invalid keys show appropriate error messages

---

## ğŸ“Š Test Scenario 5: Real-Time Streaming

**Objective**: Verify SSE streaming works correctly for long-running tasks.

**Test Prompt**:
```
Write a comprehensive guide about AI agents. Include:
- Introduction
- Types of agents
- Use cases
- Best practices
- Conclusion

Stream the response as you write it.
```

**Expected Behavior**:
- âœ… Multiple `streaming` events received
- âœ… Text appears incrementally in chat
- âœ… No connection drops or timeouts
- âœ… Final message is complete
- âœ… Typing indicator shows during streaming

**What to Check**:
- [ ] SSE connection stays open
- [ ] `streaming` events arrive continuously
- [ ] UI updates smoothly without lag
- [ ] Connection reconnects if dropped
- [ ] Final `end` event received

---

## ğŸ› ï¸ Test Scenario 6: Complex Multi-Step Workflow

**Objective**: Test a realistic complex workflow with multiple tools and agents.

**Test Prompt**:
```
I want to analyze a GitHub repository. Here's what I need:
1. Search for "camel-ai/camel" on GitHub
2. Get the repository details (stars, description, main language)
3. Clone the repository (or get README content)
4. Analyze the codebase structure
5. Create a summary document with:
   - Repository overview
   - Key features
   - Tech stack
   - Setup instructions (if available)
6. Save everything to a project folder called "camel-analysis"
```

**Expected Behavior**:
- âœ… Task is properly decomposed
- âœ… Multiple agents work together
- âœ… GitHub toolkit is used correctly
- âœ… File operations create organized folder structure
- âœ… All steps complete successfully
- âœ… Final summary is comprehensive

**What to Check**:
- [ ] `github` toolkit is activated
- [ ] `file` toolkit creates folder structure
- [ ] Multiple agents collaborate
- [ ] Task tree shows all subtasks
- [ ] Artifacts are properly organized
- [ ] No errors or timeouts

---

## âš ï¸ Test Scenario 7: Error Handling

**Objective**: Verify system handles errors gracefully.

**Test Prompts**:

**7a. Invalid Tool Usage**:
```
Try to read a file that doesn't exist: /nonexistent/path/file.txt
```

**7b. Network Error**:
```
Search the web for "test" but simulate a network failure
```

**7c. Invalid Provider**:
```
Use a provider with invalid API key and try to execute a task
```

**Expected Behavior**:
- âœ… Errors are caught and logged
- âœ… `error` SSE event is sent
- âœ… User sees friendly error message
- âœ… System doesn't crash
- âœ… Task state shows "FAILED"
- âœ… User can retry or cancel

**What to Check**:
- [ ] `error` events are properly formatted
- [ ] Error messages are user-friendly
- [ ] Failed tasks show in history
- [ ] System recovers gracefully
- [ ] No memory leaks or hanging processes

---

## ğŸ”„ Test Scenario 8: Session Management

**Objective**: Verify session persistence and history tracking.

**Test Steps**:
1. Create a new task and let it complete
2. Check History page
3. Create another task
4. Verify both sessions appear in sidebar
5. Click on old session - verify it loads correctly

**Expected Behavior**:
- âœ… Sessions are saved to database
- âœ… History page shows all sessions
- âœ… Sessions are grouped by project
- âœ… Session titles are auto-generated
- âœ… Clicking session loads chat history
- âœ… SSE events are replayed (if supported)

**What to Check**:
- [ ] POST `/chat/histories` saves session
- [ ] GET `/chat/histories` returns all sessions
- [ ] Sessions appear in sidebar
- [ ] Session details load correctly
- [ ] Token usage is tracked
- [ ] Project grouping works

---

## ğŸ¨ Test Scenario 9: Workflow Visualization

**Objective**: Verify workflow diagram displays correctly.

**Test Prompt**:
```
Create a multi-step data analysis workflow:
1. Fetch data from an API
2. Process the data
3. Generate visualizations
4. Create a report
```

**Expected Behavior**:
- âœ… Workflow diagram appears
- âœ… Task nodes show correct states
- âœ… Agent nodes show assignments
- âœ… Edges connect tasks correctly
- âœ… Status colors update in real-time
- âœ… Diagram is interactive (zoom/pan)

**What to Check**:
- [ ] ReactFlow diagram renders
- [ ] Nodes update on state changes
- [ ] Tool calls appear as nodes/edges
- [ ] Agent assignments visible
- [ ] Task tree structure correct
- [ ] No rendering errors

---

## ğŸ”Œ Test Scenario 10: MCP Integration

**Objective**: Verify MCP servers work correctly.

**Test Steps**:
1. Install an MCP server (e.g., Google Drive MCP)
2. Configure it with credentials
3. Use it in a task

**Test Prompt**:
```
Use the Google Drive MCP to list my files and create a summary document.
```

**Expected Behavior**:
- âœ… MCP server connects successfully
- âœ… MCP tools are available to agent
- âœ… Agent can use MCP tools
- âœ… Results are returned correctly
- âœ… Errors are handled gracefully

**What to Check**:
- [ ] MCP server installs correctly
- [ ] MCP tools appear in available tools
- [ ] Agent can call MCP tools
- [ ] MCP responses are formatted correctly
- [ ] Connection errors are handled

---

## ğŸ“‹ Comprehensive Integration Test

**Objective**: Test the entire system end-to-end with a realistic use case.

**Test Prompt**:
```
I'm building a Python web scraper project. Help me:

1. Research best Python web scraping libraries (requests, BeautifulSoup, Scrapy)
2. Create a project structure with:
   - requirements.txt with the libraries
   - main.py with a basic scraper template
   - README.md with setup instructions
3. Write a simple scraper that fetches the title and first paragraph from a webpage
4. Test the scraper with a sample URL
5. Create a summary document explaining what was built

Use appropriate agents and tools for each step.
```

**Expected Behavior**:
- âœ… All components work together
- âœ… Multiple agents collaborate
- âœ… Files are created correctly
- âœ… Code executes successfully
- âœ… Documentation is generated
- âœ… Final summary is comprehensive

**What to Check**:
- [ ] All SSE events flow correctly
- [ ] No errors or timeouts
- [ ] Files are created in correct structure
- [ ] Code executes and produces output
- [ ] Final summary is accurate
- [ ] Session is saved correctly
- [ ] Workflow diagram shows full flow

---

## ğŸ¯ Success Criteria Checklist

After running all tests, verify:

### Backend
- [ ] All API endpoints respond correctly
- [ ] SSE streaming works without drops
- [ ] Database persists all data
- [ ] Error handling is robust
- [ ] Provider validation works
- [ ] MCP integration functions

### Frontend
- [ ] UI updates in real-time
- [ ] Workflow diagram renders correctly
- [ ] Chat interface is responsive
- [ ] History/sessions load correctly
- [ ] Settings pages work
- [ ] Error messages are user-friendly

### Integration
- [ ] End-to-end workflows complete
- [ ] Multi-agent coordination works
- [ ] Tool calls execute correctly
- [ ] Streaming is smooth
- [ ] Sessions persist correctly
- [ ] No memory leaks or crashes

---

## ğŸ› Common Issues to Watch For

1. **SSE Connection Drops**: Check timeout settings, keep-alive
2. **Tool Execution Failures**: Verify tool permissions, paths
3. **Provider Validation Errors**: Check API keys, network connectivity
4. **UI Lag**: Monitor React re-renders, optimize SSE handling
5. **Session Loading**: Verify database queries, caching
6. **Workflow Diagram**: Check ReactFlow node updates, state management

---

## ğŸ“ Notes

- Run tests in order (they build on each other)
- Check browser console for errors
- Monitor backend logs for issues
- Test with different providers (OpenAI, Anthropic, local)
- Test with both valid and invalid inputs
- Verify error recovery mechanisms

---

**Last Updated**: January 2026
**Version**: 1.0

